# MLPerf Inference Configuration
# This is a basic configuration file for MLPerf inference benchmarks

# General settings
*.SingleStream.target_latency = 10
*.MultiStream.target_latency = 80
*.Server.target_latency = 10
*.Offline.target_latency = 1

# Loadgen settings
*.Server.enable_spec_overrides = true
*.Offline.enable_spec_overrides = true 